{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f628897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from magistral_benchmark import MagistralBenchmarkConfig, MagistralReasoningBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e362c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MagistralBenchmarkConfig(\n",
    "    tokenizer_name=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    batch_size=6,\n",
    "    min_vram_gb=8,\n",
    "    test_file=\"./italic.jsonl\",\n",
    "    max_new_tokens=500,\n",
    "    max_eval_samples=1000,\n",
    "    system_message=\"Sei un assistente utile e intelligente.\",\n",
    "    output_prefix=\"magistral_small_finetuned_reasoning\",\n",
    "    # QLoRA adapter settings\n",
    "    qlora_adapter_path=\"./path/to/your/qlora/adapter\",\n",
    "    merged_model_save_path=None, \n",
    "    # Quantization settings\n",
    "    use_quantization=True,\n",
    "    quantization_type=\"nf4\",\n",
    "    quantization_compute_dtype=\"float16\",\n",
    "    # Optimizations\n",
    "    use_flash_attention=True,\n",
    "    use_torch_compile=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892641d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = MagistralReasoningBenchmark(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, accuracy, category_stats = benchmark.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nFine-tuned reasoning benchmark completed with {accuracy:.4f} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a10335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reasoning usage\n",
    "valid_reasoning = [r for r in results if r['reasoning_length'] > 0]\n",
    "avg_reasoning_length = sum(r['reasoning_length'] for r in valid_reasoning) / len(valid_reasoning) if valid_reasoning else 0\n",
    "print(f\"Questions with reasoning: {len(valid_reasoning)}/{len(results)} ({len(valid_reasoning)/len(results)*100:.1f}%)\")\n",
    "print(f\"Average reasoning length: {avg_reasoning_length:.0f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results saved with prefix: {config.output_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b66172",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.analyse_results_by_category(category_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f755a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.show_sample_predictions(results, 10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
