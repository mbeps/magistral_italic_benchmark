{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f70bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from magistral_benchmark import MagistralBenchmarkConfig, MagistralBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bbbe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… QLoRA adapter found at: ./model/magistral_normal_v1_alt/\n"
     ]
    }
   ],
   "source": [
    "config = MagistralBenchmarkConfig(\n",
    "    tokenizer_name=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    batch_size=8,\n",
    "    min_vram_gb=8,\n",
    "    test_file=\"./data/test.jsonl\",\n",
    "    max_new_tokens=350,\n",
    "    max_eval_samples=2000,\n",
    "    system_message=\"Sei un assistente utile e intelligente.\",\n",
    "    output_prefix=\"magistral_small_finetuned\",\n",
    "    # QLoRA adapter settings\n",
    "    qlora_adapter_path=\"./model/magistral_normal_v1_alt/\",\n",
    "    merged_model_save_path=\"./merge/\",\n",
    "    # Quantization settings\n",
    "    use_quantization=True,\n",
    "    quantization_type=\"nf4\",\n",
    "    quantization_compute_dtype=\"float16\",\n",
    "    # Optimizations\n",
    "    use_flash_attention=True,\n",
    "    use_torch_compile=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CUDA optimizations enabled (TF32, cuDNN benchmark)\n",
      "âœ… CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "VRAM: 23.6 GB\n",
      "âœ… GPU has sufficient VRAM for the model\n"
     ]
    }
   ],
   "source": [
    "benchmark = MagistralBenchmark(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a87d902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MAGISTRAL BENCHMARK\n",
      "ðŸ”„ WITH QLORA ADAPTER SUPPORT\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "MAGISTRAL BENCHMARK CONFIGURATION\n",
      "ðŸ”„ QLoRA ADAPTER SUPPORT ENABLED\n",
      "============================================================\n",
      "âœ“ Model: mistralai/Magistral-Small-2506\n",
      "âœ“ Tokenizer: mistralai/Mistral-Nemo-Instruct-2407\n",
      "âœ“ QLoRA Adapter: ./model/magistral_normal_v1_alt/\n",
      "âœ“ Save merged model to: ./merge/\n",
      "âœ“ Test file: ./data/test.jsonl\n",
      "âœ“ Max samples: 2000\n",
      "âœ“ Batch size: 8\n",
      "âœ“ Max new tokens: 350\n",
      "âœ“ Min VRAM required: 8GB\n",
      "âœ“ Quantization: nf4 (float16)\n",
      "âœ“ Flash Attention 2: Enabled\n",
      "âœ“ torch.compile: Enabled\n",
      "âœ“ Output prefix: magistral_small_finetuned\n",
      "âœ“ System message: Sei un assistente utile e intelligente.\n",
      "============================================================\n",
      "\n",
      "ðŸ“š Loading ITALIC dataset...\n",
      "Loaded 10000 questions\n",
      "\n",
      "Original dataset category distribution:\n",
      "  art_history: 980 questions\n",
      "  civic_education: 973 questions\n",
      "  current_events: 92 questions\n",
      "  geography: 979 questions\n",
      "  history: 978 questions\n",
      "  lexicon: 979 questions\n",
      "  literature: 984 questions\n",
      "  morphology: 140 questions\n",
      "  orthography: 971 questions\n",
      "  synonyms_and_antonyms: 971 questions\n",
      "  syntax: 973 questions\n",
      "  tourism: 980 questions\n",
      "\n",
      "Creating uniform subset with 2000 samples:\n",
      "  art_history: selected 167 / 980 questions\n",
      "  civic_education: selected 167 / 973 questions\n",
      "  current_events: selected 92 / 92 questions\n",
      "  geography: selected 167 / 979 questions\n",
      "  history: selected 167 / 978 questions\n",
      "  lexicon: selected 167 / 979 questions\n",
      "  literature: selected 167 / 984 questions\n",
      "  morphology: selected 140 / 140 questions\n",
      "  orthography: selected 166 / 971 questions\n",
      "  synonyms_and_antonyms: selected 166 / 971 questions\n",
      "  syntax: selected 166 / 973 questions\n",
      "  tourism: selected 166 / 980 questions\n",
      "\n",
      "Final subset: 1898 questions\n",
      "\n",
      "Using 1898 questions for evaluation\n",
      "Categories: {'art_history': 167, 'civic_education': 167, 'current_events': 92, 'geography': 167, 'history': 167, 'lexicon': 167, 'literature': 167, 'morphology': 140, 'orthography': 166, 'synonyms_and_antonyms': 166, 'syntax': 166, 'tourism': 166}\n",
      "ðŸ”„ QLoRA adapter specified - will merge with base model first\n",
      "\n",
      "ðŸ”„ Merging QLoRA adapters with base model...\n",
      "Adapter path: ./model/magistral_normal_v1_alt/\n",
      "Base model: mistralai/Magistral-Small-2506\n",
      "ðŸ“¦ Loading QLoRA adapter and base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b7661807fa427fbebb6c5a37c821e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PEFT model loaded\n",
      "ðŸ”„ Merging QLoRA adapters into base model...\n",
      "âœ… Adapters merged\n",
      "ðŸ’¾ Saving merged model to: ./merge/\n",
      "âœ… Tokenizer saved with merged model\n",
      "âœ… QLoRA adapters successfully merged!\n",
      "ðŸ”„ Loading mistralai/Magistral-Small-2506 + QLoRA adapter...\n",
      "Loading tokenizer: mistralai/Mistral-Nemo-Instruct-2407\n",
      "âš ï¸ Flash Attention 2 not available - using standard attention\n",
      "Loading model with optimizations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa326b670d2440ab97aacd299e993496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model with torch.compile for optimization...\n",
      "âœ… Model compiled successfully!\n",
      "âœ… Model loaded successfully!\n",
      "Model device: cuda:0\n",
      "Model dtype: torch.float16\n",
      "GPU Memory - Allocated: 13.19 GB, Available: 10.35 GB\n",
      "ðŸš€ Optimal batch size determined: 8\n",
      "\n",
      "ðŸ§ª Testing inference...\n",
      "Question: Che tipo di proposizione contiene la frase: FarÃ² tutto ciÃ² che desideri, purchÃ© tu sia felice, perci...\n",
      "Test response: 'D'\n",
      "Expected answer: 'D'\n",
      "Extracted answer: 'D'\n",
      "Correct: True\n",
      "\n",
      "==================================================\n",
      "STARTING EVALUATION\n",
      "==================================================\n",
      "\n",
      "ðŸ” Evaluating mistralai/Magistral-Small-2506 + QLoRA (magistral_normal_v1_alt) on 1898 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 238/238 [07:42<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š FINAL RESULTS:\n",
      "Total questions: 1898\n",
      "Correct answers: 1413\n",
      "Accuracy: 0.7445 (74.45%)\n",
      "\n",
      "ðŸ“ˆ RESULTS BY CATEGORY:\n",
      "------------------------------------------------------------\n",
      "Category                      Accuracy  Correct    Total\n",
      "------------------------------------------------------------\n",
      "art_history                     73.05%      122      167\n",
      "civic_education                 77.84%      130      167\n",
      "current_events                  80.43%       74       92\n",
      "geography                       84.43%      141      167\n",
      "history                         78.44%      131      167\n",
      "lexicon                         89.82%      150      167\n",
      "literature                      68.86%      115      167\n",
      "morphology                      47.14%       66      140\n",
      "orthography                     67.47%      112      166\n",
      "synonyms_and_antonyms           91.57%      152      166\n",
      "syntax                          59.64%       99      166\n",
      "tourism                         72.89%      121      166\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ” SAMPLE PREDICTIONS:\n",
      "\n",
      "Example 1 âœ… CORRECT:\n",
      "  Category: syntax\n",
      "  Question: Che tipo di proposizione contiene la frase: FarÃ² tutto ciÃ² che desideri, purchÃ© tu sia felice, perci...\n",
      "  Expected: D\n",
      "  Predicted: D\n",
      "  Raw output: 'D...'\n",
      "\n",
      "Example 2 âœ… CORRECT:\n",
      "  Category: syntax\n",
      "  Question: Nella frase \"Luca resterÃ  a Roma per dieci giorni\", si trovano:...\n",
      "  Expected: B\n",
      "  Predicted: B\n",
      "  Raw output: 'B...'\n",
      "\n",
      "Example 3 âŒ INCORRECT:\n",
      "  Category: art_history\n",
      "  Question: Quale di queste affermazioni sul ciclo pittorico sistino che decora la Scala Santa in Roma non Ã¨ ver...\n",
      "  Expected: B\n",
      "  Predicted: D\n",
      "  Raw output: 'D...'\n",
      "\n",
      "Example 4 âŒ INCORRECT:\n",
      "  Category: morphology\n",
      "  Question: Nel termine trasmettere il gruppo di lettere \"tras-\" Ã¨:...\n",
      "  Expected: A\n",
      "  Predicted: B\n",
      "  Raw output: 'B...'\n",
      "\n",
      "ðŸ’¾ Saving results...\n",
      "Detailed results saved to 'magistral_small_finetuned_results.csv'\n",
      "Summary saved to 'magistral_small_finetuned_summary.json'\n",
      "\n",
      "ðŸŽ‰ EVALUATION COMPLETED!\n",
      "============================================================\n",
      "ðŸ“Š Model: mistralai/Magistral-Small-2506 + QLoRA (magistral_normal_v1_alt)\n",
      "ðŸ“Š Final accuracy: 0.7445 (74.45%)\n",
      "ðŸ“Š Total questions evaluated: 1898\n",
      "ðŸ“Š Batch size used: 8\n",
      "ðŸ“Š QLoRA adapter merged and evaluated\n",
      "============================================================\n",
      "\n",
      "ðŸ§¹ Final cleanup...\n",
      "Final GPU memory usage: 0.01GB\n",
      "âœ… mistralai/Magistral-Small-2506 + QLoRA (magistral_normal_v1_alt) benchmark complete! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "results, accuracy, category_stats = benchmark.run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2f54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuned benchmark completed with 0.7445 accuracy\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFine-tuned benchmark completed with {accuracy:.4f} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4a071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved with prefix: magistral_small_finetuned\n"
     ]
    }
   ],
   "source": [
    "print(f\"Results saved with prefix: {config.output_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8035b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ RESULTS BY CATEGORY:\n",
      "------------------------------------------------------------\n",
      "Category                      Accuracy  Correct    Total\n",
      "------------------------------------------------------------\n",
      "art_history                     73.05%      122      167\n",
      "civic_education                 77.84%      130      167\n",
      "current_events                  80.43%       74       92\n",
      "geography                       84.43%      141      167\n",
      "history                         78.44%      131      167\n",
      "lexicon                         89.82%      150      167\n",
      "literature                      68.86%      115      167\n",
      "morphology                      47.14%       66      140\n",
      "orthography                     67.47%      112      166\n",
      "synonyms_and_antonyms           91.57%      152      166\n",
      "syntax                          59.64%       99      166\n",
      "tourism                         72.89%      121      166\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "benchmark.analyse_results_by_category(category_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” SAMPLE PREDICTIONS:\n",
      "\n",
      "Example 1 âœ… CORRECT:\n",
      "  Category: syntax\n",
      "  Question: Che tipo di proposizione contiene la frase: FarÃ² tutto ciÃ² che desideri, purchÃ© tu sia felice, perci...\n",
      "  Expected: D\n",
      "  Predicted: D\n",
      "  Raw output: 'D...'\n",
      "\n",
      "Example 2 âœ… CORRECT:\n",
      "  Category: syntax\n",
      "  Question: Nella frase \"Luca resterÃ  a Roma per dieci giorni\", si trovano:...\n",
      "  Expected: B\n",
      "  Predicted: B\n",
      "  Raw output: 'B...'\n",
      "\n",
      "Example 3 âœ… CORRECT:\n",
      "  Category: synonyms_and_antonyms\n",
      "  Question: â€œConvocareâ€ Ã¨ un contrario di:...\n",
      "  Expected: A\n",
      "  Predicted: A\n",
      "  Raw output: 'A...'\n",
      "\n",
      "Example 4 âœ… CORRECT:\n",
      "  Category: lexicon\n",
      "  Question: Qual Ã¨ il significato di â€œpesanteâ€ nel contesto: Lâ€™atmosfera si era fatta pesante?...\n",
      "  Expected: A\n",
      "  Predicted: A\n",
      "  Raw output: 'A...'\n",
      "\n",
      "Example 5 âœ… CORRECT:\n",
      "  Category: art_history\n",
      "  Question: In che anno venne distribuito il film d'animazione della Walt Disney Production \"Cenerentola\"?...\n",
      "  Expected: B\n",
      "  Predicted: B\n",
      "  Raw output: 'B...'\n",
      "\n",
      "Example 6 âŒ INCORRECT:\n",
      "  Category: art_history\n",
      "  Question: Quale di queste affermazioni sul ciclo pittorico sistino che decora la Scala Santa in Roma non Ã¨ ver...\n",
      "  Expected: B\n",
      "  Predicted: D\n",
      "  Raw output: 'D...'\n",
      "\n",
      "Example 7 âŒ INCORRECT:\n",
      "  Category: morphology\n",
      "  Question: Nel termine trasmettere il gruppo di lettere \"tras-\" Ã¨:...\n",
      "  Expected: A\n",
      "  Predicted: B\n",
      "  Raw output: 'B...'\n",
      "\n",
      "Example 8 âŒ INCORRECT:\n",
      "  Category: syntax\n",
      "  Question: Qual Ã¨ la proposizione oggettiva nella frase: UscirÃ² verso le cinque perchÃ© penso di comprare il lib...\n",
      "  Expected: C\n",
      "  Predicted: D\n",
      "  Raw output: 'D...'\n",
      "\n",
      "Example 9 âŒ INCORRECT:\n",
      "  Category: art_history\n",
      "  Question: In architettura indica eremi, chiese e monasteri nell'Italia meridionale, sorti tra l'800 e il 1300 ...\n",
      "  Expected: A\n",
      "  Predicted: C\n",
      "  Raw output: 'C...'\n",
      "\n",
      "Example 10 âŒ INCORRECT:\n",
      "  Category: syntax\n",
      "  Question: â€œAnche se non vinse, ma morÃ¬ in duello, il personaggio di Ettore Ã¨ certamente il piÃ¹ bello di tutta ...\n",
      "  Expected: A\n",
      "  Predicted: D\n",
      "  Raw output: 'D...'\n"
     ]
    }
   ],
   "source": [
    "benchmark.show_sample_predictions(results, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
